---
title: "Lecture: linear modeling for microbiome data in R/Bioconductor"
author: "Levi Waldron"
date: "June 30, 2017"
output:
  slidy_presentation: default
---

```{r setup, cache=TRUE, echo=FALSE}
library(knitr)
# global chunk options
opts_chunk$set(cache=TRUE, autodep=TRUE)
```

## Outline

* Multiple linear regression
    + Continuous and categorical predictors
    + Interactions
* Model formulae
* Design matrix
* Generalized Linear Models
    + Linear, logistic, log-Linear links
    + Poisson, Negative Binomial error distributions
    + Zero inflation


## Linear modeling for metagenomic data: Two main approaches (1)

1. normalizing transformation, orinary linear modeling
     - calculate relative abundance, dividing by the total number of counts for each sample (account for different sequencing depths)
     - variance-stabilizing transformation of features, `arcsin(sqrt(x))`

* *Advantages*
    - simplicity: can directly use PCA, linear models, non-parametric tests

* *Disadvantages*
    - data may still not be very normally distributed
    - regression coefficients for arcsin-sqrt transformed data not easily interpretable

## Two main approaches (2)

2. treat as count data, log-linear generalized linear model (GLM)
     - log-linear systematic component
     - typically negative binomially-distributed random component
     - model can include an "offset" term to account for different sequencing depths

* *Advantages*
    - GLM framework provides great flexibility to deal with sequencing depth, over-dispersion
    - coefficients are readily interpretable in "multiplicative" models
    - `phyloseq` and `DESeq2` packages simplify the process

* *Disadvantages*
    - models are more complicated

# Multiple Linear Regression Model (approach 1)

## Example: friction of spider legs

* Wolff & Gorb, [Radial arrangement of Janus-like setae permits friction control in spiders](http://www.nature.com/articles/srep01101), *Sci. Rep.* 2013.

<div class="columns-2">
<center> <img src="srep01101-f4.jpg" height=600> </center>

- **(A)** Barplot showing total claw tuft area of the corresponding legs. 
- **(B)** Boxplot presenting friction coefficient data illustrating median, interquartile range and extreme values.
</div>

## Example: friction of spider legs

<div class="columns-2">
<center> <img src="srep01101-f4.jpg" height=600> </center>

- Are the pulling and pushing friction coefficients different?
- Are the friction coefficients different for the different leg pairs?
- Does the difference between pulling and pushing friction coefficients vary by leg pair?
</div>

```{r, echo=FALSE}
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/spider_wolff_gorb_2013.csv"
filename <- "spider_wolff_gorb_2013.csv"
library(downloader)
if (!file.exists(filename))
    download(url, filename)
spider <- read.csv(filename, skip=1)
```

## Example: friction of spider legs

```{r}
table(spider$leg,spider$type)
summary(spider)
```

## Example: friction of spider legs

```{r, fig.align='center'}
boxplot(spider$friction ~ spider$type * spider$leg,
        col=c("grey90","grey40"), las=2,
        main="Friction coefficients of different leg pairs")
```

## Example: friction of spider legs

<div class="columns-2">

```{r, fig.align='center', echo=FALSE}
boxplot(spider$friction ~ spider$type * spider$leg,
        col=c("grey90","grey40"), las=2,
        main="Friction coefficients of \n different leg pairs")
```

Notes:

- Pulling friction is higher
- Pulling (but not pushing) friction increases for further back legs (L1 -> 4)
- Variance isn't constant

</div>

## What are linear models?

The following are examples of linear models:

1. $Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$ (simple linear regression)
2. $Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon_i$ (quadratic regression)
3. $Y_i = \beta_0 + \beta_1 x_i + \beta_2 \times 2^{x_i} + \varepsilon_i$ (2^{x_i} is a new transformed variable)

## Multiple linear regression model

- Linear models can have any number of predictors
- Systematic part of model:

$$
E[y|x] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p
$$

- $E[y|x]$ is the expected value of $y$ given $x$
- $y$ is the outcome, response, or dependent variable
- $x$ is the vector of predictors / independent variables 
- $x_p$ are the individual predictors or independent variables
- $\beta_p$ are the regression coefficients

## Multiple linear regression model

Random part of model:

$y_i = E[y_i|x_i] + \epsilon_i$

Assumptions of linear models: $\epsilon_i \stackrel{iid}{\sim} N(0, \sigma_\epsilon^2)$

* Normal distribution
* Mean zero at every value of predictors
* Constant variance at every value of predictors
* Values that are statistically independent

## Continuous predictors

* **Coding:** as-is, or may be scaled to unit variance (which results in _adjusted_ regression coefficients)
* **Interpretation for linear regression:** An increase of one unit of the predictor results in this much difference in the continuous outcome variable

## Binary predictors (2 levels)

* **Coding:** indicator or dummy variable (0-1 coding)
* **Interpretation for linear regression:** the increase or decrease in average outcome levels in the group coded “1”, compared to the reference category (“0”)
   + _e.g._ $E(y|x) = \beta_0 + \beta_1 x$ 
   + where x={ 1 if push friction, 0 if pull friction }

## Multilevel categorical predictors (ordinal or nominal)

* **Coding:** $K-1$ dummy variables for $K$-level categorical variable
* Comparisons with respect to a reference category, *e.g.* `L1`:
    * `L2`={1 if $2^{nd}$ leg pair, 0 otherwise}, 
    * `L3`={1 if $3^{nd}$ leg pair, 0 otherwise}, 
    * `L4`={1 if $4^{th}$ leg pair, 0 otherwise}.

- R re-codes factors to dummy variables automatically. 
- Dummy coding depends on whether factor is ordered or not.

# Model formulae

## Model formulae in R

[Model formulae tutorial](http://ww2.coastal.edu/kingw/statistics/R-tutorials/formulae.html)

* regression functions in R such as `aov()`, `lm()`, `glm()`, and `coxph()` use a "model formula" interface.
* The formula determines the model that will be built (and tested) by the R procedure. The basic format is:

`> response variable ~ explanatory variables`

* The tilde means "is modeled by" or "is modeled as a function of." 

## Regression with a single predictor

Model formula for simple linear regression: 

`> y ~ x`

* where "x" is the explanatory (independent) variable
* "y" is the response (dependent) variable. 

## Return to the spider legs

Friction coefficient for leg type of first leg pair:

```{r, results='show'}
spider.sub <- spider[spider$leg=="L1", ]
fit <- lm(friction ~ type, data=spider.sub)
summary(fit)
```

## Regression on spider leg type

Regression coefficients for `friction ~ type` for first set of spider legs:

```{r, results="asis", echo=TRUE}
fit.table <- xtable::xtable(fit, label=NULL)
print(fit.table, type="html")
```

<p></p>
* How to interpret this table?
    * Coefficients for **(Intercept)** and **typepush**
    * Coefficients are t-distributed when assumptions are correct
    * Variance in the estimates of each coefficient can be calculated

## Interpretation of spider leg type coefficients

```{r spider_main_coef, fig.cap="Diagram of the estimated coefficients in the linear model. The green arrow indicates the Intercept term, which goes from zero to the mean of the reference group (here the 'pull' samples). The orange arrow indicates the difference between the push group and the pull group, which is negative in this example. The circles show the individual samples, jittered horizontally to avoid overplotting.",echo=FALSE}
set.seed(1) #same jitter in stripchart
stripchart(split(spider.sub$friction, spider.sub$type), 
           vertical=TRUE, pch=1, method="jitter", las=2, xlim=c(0,3), ylim=c(0,2))
coefs <- coef(fit)
a <- -0.25
lgth <- .1
library(RColorBrewer)
cols <- brewer.pal(3,"Dark2")
abline(h=0)
arrows(1+a,0,1+a,coefs[1],lwd=3,col=cols[1],length=lgth)
abline(h=coefs[1],col=cols[1])
arrows(2+a,coefs[1],2+a,coefs[1]+coefs[2],lwd=3,col=cols[2],length=lgth)
abline(h=coefs[1]+coefs[2],col=cols[2])
legend("right",names(coefs),fill=cols,cex=.75,bg="white")
```

## regression on spider leg **position**

Remember there are positions 1-4
```{r}
fit <- lm(friction ~ leg, data=spider)
```

```{r, results="asis", echo=TRUE, message=FALSE}
fit.table <- xtable::xtable(fit, label=NULL)
print(fit.table, type="html")
```

- Interpretation of the dummy variables legL2, legL3, legL4 ?

## Regression with multiple predictors

Additional explanatory variables can be added as follows: 

`> y ~ x + z`

Note that "+" does not have its usual meaning, which would be achieved by:

`> y ~ I(x + z)`

## Regression on spider leg **type** and **position**

Remember there are positions 1-4
```{r}
fit <- lm(friction ~ type + leg, data=spider)
```

```{r, results="asis", echo=TRUE, message=FALSE}
fit.table <- xtable::xtable(fit, label=NULL)
print(fit.table, type="html")
```

* this model still doesn't represent how the friction differences between different leg positions are modified by whether it is pulling or pushing

## Interaction (effect modification)

Interaction is modeled as the product of two covariates:
$$
E[y|x] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_1*x_2
$$

## Model formulae (cont'd)

symbol  | example | meaning
------- | ------------ | --------------------------  
+ | + x	| include this variable  
-	| - x	| delete this variable  
:	| x : z	| include the interaction  
*	| x * z	| include these variables and their interactions  
^	| (u + v + w)^3	| include these variables and all interactions up to three way
1 | -1 | intercept: delete the intercept  

Note: order generally doesn't matter (u+v OR v+u)

## Summary: types of standard linear models

```
lm( y ~ u + v)
```
`u` and `v` factors: **ANOVA**  
`u` and `v` numeric: **multiple regression**  
one factor, one numeric: **ANCOVA**

* R does a lot for you based on your variable classes
    * be **sure** you know the classes of your variables
    * be sure all rows of your regression output make sense

# The Design Matrix

## The Design Matrix

Recall the multiple linear regression model:

$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_p x_{pi} + \epsilon_i$

- $x_{ji}$ is the value of predictor $x_j$ for observation $i$

## The Design Matrix

Matrix notation for the multiple linear regression model:

$$
\,
\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix} = 
\begin{pmatrix}
1&x_1\\
1&x_2\\
\vdots\\
1&x_N
\end{pmatrix}
\begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix} +
\begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_N
\end{pmatrix}
$$

or simply: 

$$
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}
$$

* The design matrix is $\mathbf{X}$
    * which the computer will take as a given when solving for $\boldsymbol{\beta}$ by minimizing the sum of squares of residuals $\boldsymbol{\varepsilon}$.
    
## Choice of design matrix
    
* there are multiple possible and reasonable design matrices for a given study design
* the model formula encodes a default model matrix, e.g.:

```{r}
group <- factor( c(1, 1, 2, 2) )
model.matrix(~ group)
```

## Choice of design matrix

What if we forgot to code group as a factor?
```{r}
group <- c(1, 1, 2, 2)
model.matrix(~ group)
```

## More groups, still one variable

```{r}
group <- factor(c(1,1,2,2,3,3))
model.matrix(~ group)
```

## Changing the baseline group

```{r}
group <- factor(c(1,1,2,2,3,3))
group <- relevel(x=group, ref=3)
model.matrix(~ group)
```

## More than one variable

```{r}
diet <- factor(c(1,1,1,1,2,2,2,2))
sex <- factor(c("f","f","m","m","f","f","m","m"))
model.matrix(~ diet + sex)
```

## With an interaction term

```{r}
model.matrix(~ diet + sex + diet:sex)
```

## Summary: applications of model matrices

* Major differential expression packages recognize them:
    * LIMMA (VOOM for RNA-seq)
    * DESeq2 for all kinds of count data
    * EdgeR
* Can fit coefficients **directly** to your contrast of interest
    * *e.g.*: what is the difference between push/pull friction for each spider-leg pair?

# Generalized Linear Models (approach 2)

## Generalized Linear Models

* Linear regression is a special case of a broad family of models called "Generalized Linear Models" (GLM)
* This unifying approach allows to fit a large set of models using maximum likelihood estimation methods (MLE) (Nelder & Wedderburn, 1972)
* Can model many types of data directly using appropriate random distribution and "link" function
    + Transformations of $Y$ not needed

## Components of GLM

* **Random component** specifies the conditional distribution for the response variable
    + *e.g.* normal, Poisson, Negative Binomial...

* **Systematic component** specifies linear function of predictors (linear predictor)
* **Link** [denoted by g(.)] specifies the relationship between the expected value of the random component and the systematic component
    + can be linear or nonlinear  

## Log-linear models

Systematic component is:

$$
log(E[y|x_i]) = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_p x_{pi}
$$

Or equivalently:
$$
E[y|x_i] = exp \left( \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_p x_{pi} \right)
$$

where $E[y|x_i]$ is the expected number of counts for a microbe in subject _i_

* Systematic plus random components:

$\epsilon_i$ is typically Poisson or Negative Binomal distributed.


* Note: Modeling $log(E[y|x_i])$ is _not_ equivalent to modeling $E(log(y|x_i))$

## Additive vs. Multiplicative models

* Linear regression is an _additive_ model
    + _e.g._ for two binary variables $\beta_1 = 1.5$, $\beta_2 = 1.5$.
    + If $x_1=1$ and $x_2=1$, this adds 3.0 to $E(y|x)$

* Log-linear models are _multiplicative_:
    + If $x_1=1$ and $x_2=1$, this adds 3.0 to $log(E[y_i])$
    + Expected count increases 20-fold: $exp(1.5+1.5)$ or $exp(1.5) * exp(1.5)$
    + Coefficients are invariant to multiplicative scaling of the data

This is a very important distinction!

## Demystifying error models

* In regression we model observations as coming from a **random** distribution with **fixed** parameters:
    + linear regression: *normal* distribution, with *mean* and *standard deviation*
    + log-linear models: *Poisson* distribution with *mean* $\lambda$, or *Negative Binomial* distribution with parameters $n$ and $p$

If there is evidence that the fixed parameters differ between two groups of interest, we say the results are *statistically significant*.


## Poisson model

* In the Poisson distribution, the variance is equal to the mean.
* _i.e._ if the mean number of a microbe across all samples is 4, then variance is also 4 and the standard deviation is 2.
* The Poisson distribution fails when the variance exceeds the mean

## Visualizing the Poisson Distribution

* Poisson distribution has one parameter:
    + mean $\lambda$ is greater than 0
    + variance is also $\lambda$

```{r, echo=FALSE}
par(mar=c(4, 4, 0, 0))
plot(x=0:10, y=dpois(0:10, lambda=1), 
     type="b", lwd=2,
     xlab="Counts (k)", ylab="Probability density")
lines(x=0:10, y=dpois(0:10, lambda=2), 
      type="b", lwd=2, lty=2, pch=2)
lines(x=0:10, dpois(0:10, lambda=4), 
      type="b", lwd=2, lty=3, pch=3)
legend("topright", lwd=2, lty=1:3, pch=1:3,
       legend=c(expression(paste(lambda, "=1")),
                expression(paste(lambda, "=2")),
                expression(paste(lambda, "=4"))))
```

## Negative binomial distribution

* The binomial distribution is the number of successes in n trials:
    + Roll a die ten times, how many times do you see a 6?
* The negative binomial distribution is the number of successes it takes to observe r failures:
    + How many times do you have to roll the die to see a 6 ten times?
    + Note that the number of rolls is no longer fixed.
    + In this example, p=5/6 and a 6 is a "failure"

## Visualizing the Negative Binomial Distribution

```{r, echo=FALSE}
plot(x=0:40, y=dnbinom(0:40, size=10, prob=0.5), 
     type="b", lwd=2, ylim=c(0, 0.2),
     xlab="Counts (k)", ylab="Probability density")
lines(x=0:40, y=dnbinom(0:40, size=20, prob=0.5), 
      type="b", lwd=2, lty=2, pch=2)
lines(x=0:40, y=dnbinom(0:40, size=10, prob=0.3),
      type="b", lwd=2, lty=3, pch=3)
legend("topright", lwd=2, lty=1:3, pch=1:3,
       legend=c("n=10, p=0.5", "n=20, p=0.5", "n=10, p=0.3"))
```

## Compare Poisson vs. Negative Binomial

Negative Binomial Distribution has two parameters: # of trials n, and probability of success p

```{r, echo=FALSE}
plot(x=0:40, y=dnbinom(0:40, size=10, prob=0.5), 
     type="b", lwd=2, ylim=c(0, 0.15),
     xlab="Counts (k)", ylab="Probability density")
lines(x=0:40, y=dnbinom(0:40, size=20, prob=0.5), 
      type="b", lwd=2, lty=2, pch=2)
lines(x=0:40, y=dnbinom(0:40, size=10, prob=0.3),
      type="b", lwd=2, lty=3, pch=3)
lines(x=0:40, y=dpois(0:40, lambda=9), col="red")
lines(x=0:40, y=dpois(0:40, lambda=20), col="red")
legend("topright", lwd=c(2,2,2,1), lty=c(1:3,1), pch=c(1:3,-1), col=c(rep("black", 3), "red"),
       legend=c("n=10, p=0.5", "n=20, p=0.5", "n=10, p=0.3", "Poisson"))
```

## Zero-inflated models

* Two-step model:
    1. logistic model to determine whether count is zero or Poisson/NB
    2. Poisson or NB regression distribution for $y_i$ not set to zero by step *1.*

* Not currently supported by DESeq2, edgeR, limma (as far as I know)
    + but supported by `metagenomeSeq`
* *Warning:* be aware what your logistic model is
    + best to keep it intercept-only

## Poisson Distribution with Zero Inflation

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(gamlss)
##par(cex=2)  #increase size of type and axes
plot(x=0:10, y=dpois(0:10, lambda=2), 
     type="b", lwd=2, ylim=c(0, 0.5),
     xlab="Counts (k)", ylab="Probability density")
lines(x=0:10, y=dZIP(0:10, mu=2, sigma=0.2),
      type="b", lwd=2, lty=2, pch=2)
lines(x=0:10, y=dZIP(0:10, mu=2, sigma=0.4),
      type="b", lwd=2, lty=3, pch=3)
legend("topright", lwd=2, lty=1:3, pch=1:3,
       legend=c(expression(paste(lambda, "=2")),
                expression(paste("ZIP: ", lambda, "=2, ", "p=0.2")),
                expression(paste("ZIP: ", lambda, "=2, ", "p=0.4"))))
```

## Summary

- Log-linear GLMs are preferred for inference from 16S rRNA and shotgun metagenomic data
- Be aware of both models being fitted in a zero-inflated mixture / hurdle model
    + a log-linear model and a logistic model
    + keep the latter intercept-only unless you have good reason to do otherwise
    
## Links

- A built [html][] version of this lecture is available.
- The [source][] R Markdown is also available from Github.

[html]: http://rpubs.com/lwaldron/EPIC2017_linearmodelingecture
[source]: https://github.com/waldronlab/presentations/tree/master/Waldron_2017-06-30_EPIC
